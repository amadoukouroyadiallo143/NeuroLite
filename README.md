# NeuroLite

Une architecture d'IA l√©g√®re pour les appareils mobiles et embarqu√©s, fournissant des alternatives efficaces aux Transformers. NeuroLite impl√©mente des approches innovantes (MLP-Mixer, m√©moire neuronale, routage adaptatif) pour cr√©er des mod√®les compacts capables de traiter le langage naturel avec une fraction des ressources requises par les architectures traditionnelles.

<div align="center">
<strong>Complexit√© Lin√©aire | Empreinte Minimale | AGI L√©g√®re</strong>
</div>

## üåü Points Cl√©s

- **Ultra-l√©ger**: Mod√®les de 1-10Mo, contre 110-340Mo pour les Transformers standards
- **Efficacit√© Computationnelle**: Complexit√© lin√©aire (O(n)) en longueur de s√©quence vs quadratique (O(n¬≤)) 
- **M√©moire Adaptative**: Syst√®me de r√©tention contextuelle √† long terme inspir√© des r√©seaux de Hopfield modernes
- **Routage Intelligent**: Activation conditionnelle des experts sp√©cialis√©s selon le type d'entr√©e
- **Composant Symbolique**: Module l√©ger de raisonnement structur√© pour am√©liorer les capacit√©s symboliques
- **Mobile-First**: Con√ßu pour fonctionner efficacement sur smartphones, wearables et dispositifs IoT

## üèóÔ∏è Architecture

NeuroLite combine plusieurs innovations r√©centes en une architecture hybride l√©g√®re et performante:

![Architecture NeuroLite](https://placeholder-for-architecture-diagram.com/neurolite_arch.png)

1. **Projection d'entr√©e efficace** - Remplace les lourdes tables d'embedding par un encodage l√©ger bas√© sur MinHash et filtres de Bloom (~99% de r√©duction de param√®tres)
2. **Backbone All-MLP** - Couches MLP-Mixer ou HyperMixer pour un traitement de s√©quence avec complexit√© temporelle et spatiale lin√©aire
3. **M√©moire externe diff√©rentiable** - Syst√®me de m√©moire associative √† plusieurs niveaux pour la r√©tention contextuelle
4. **Composants Neurosymboliques Avanc√©s**:
    - **Moteur de R√®gles Symboliques (`SymbolicRuleEngine`)**: Un moteur d'inf√©rence l√©ger qui charge des r√®gles (logique de premier ordre) et des faits √† partir de fichiers JSON. Il supporte l'ajout dynamique de faits, l'inf√©rence vers l'avant (forward chaining) et la gestion de la n√©gation dans les pr√©misses des r√®gles.
    - **Couche Neurosymbolique (`NeuralSymbolicLayer`)**: Int√®gre le `SymbolicRuleEngine` dans le pipeline neuronal. Cette couche peut extraire des entit√©s et relations potentielles des √©tats cach√©s du mod√®le, les affirmer comme faits transitoires au moteur de r√®gles, initier une phase d'inf√©rence, puis r√©int√©grer les faits d√©riv√©s dans les repr√©sentations neuronales. Elle supporte le traitement par batch et utilise des embeddings apprenables pour les types de pr√©dicats et les identifiants d'entit√©s symboliques, permettant au mod√®le d'apprendre √† interpr√©ter et utiliser les r√©sultats du raisonnement symbolique. Le fichier `rules.json` (configurable via `symbolic_rules_file` dans `NeuroLiteConfig`) est utilis√© pour charger les r√®gles et faits persistants.
    - **R√©seau Bay√©sien (`BayesianBeliefNetwork`)**: Permet d'int√©grer des connaissances probabilistes et d'effectuer du raisonnement incertain. La structure du r√©seau (variables et leurs d√©pendances) peut √™tre d√©finie via la configuration (`bayesian_network_structure` et `num_bayesian_variables` dans `NeuroLiteConfig`). Le module utilise un algorithme d'inf√©rence approximative (bas√© sur le Likelihood Weighting) pour estimer les probabilit√©s post√©rieures √©tant donn√© des √©vidences extraites des √©tats neuronaux.
5. **Routage dynamique** - Activation conditionnelle de sous-modules sp√©cialis√©s via Mixture-of-Experts l√©ger
6. **Apprentissage Continu et M√©moire √âvolu√©e**:
    - **Adaptateur d'Apprentissage Continu (`ContinualAdapter`)**: Int√©gr√© au mod√®le, ce module vise √† permettre l'apprentissage √† partir de nouvelles donn√©es au fil du temps tout en att√©nuant l'oubli catastrophique des connaissances ant√©rieures. Il utilise des m√©canismes comme un tampon de rejeu (`replay buffer`) pour stocker des exp√©riences pass√©es, une d√©tection conceptuelle de d√©rive de distribution (`drift detection`), et des strat√©gies d'adaptation du mod√®le. Voir `examples/lifelong_learning_demo.py`.
    - **M√©moire Hi√©rarchique Am√©lior√©e (`HierarchicalMemory`)**: La m√©moire hi√©rarchique a √©t√© dot√©e de capacit√©s plus dynamiques :
        - **Consolidation Intelligente**: Le transfert d'informations entre les niveaux de m√©moire (court terme, long terme, persistant) est d√©sormais modul√© par la nouveaut√© des donn√©es. Les informations nouvelles et surprenantes sont prioris√©es pour la consolidation dans les m√©moires √† plus long terme, rendant les mises √† jour plus s√©lectives et efficaces. Ceci est contr√¥l√© par `novelty_threshold_ltm` et `novelty_threshold_pm`.
        - **Portes Contextuelles de M√©moire**: La contribution de chaque niveau de m√©moire (STM, LTM, PM) √† la sortie finale est d√©termin√©e dynamiquement pour chaque token d'entr√©e, gr√¢ce √† des poids calcul√©s par `memory_gate` sur les requ√™tes. Cela permet une r√©cup√©ration d'informations contextuelles plus nuanc√©e et pertinente.

## üß† Fondements Th√©oriques

NeuroLite s'inspire de plusieurs avanc√©es th√©oriques r√©centes:

- **MLP-Mixer**: D√©montre que des projections lin√©aires altern√©es (token-mixing et channel-mixing) peuvent rivaliser avec l'attention pour de nombreuses t√¢ches
- **Complexit√© Lin√©aire**: Exploite les approches comme Performer, Linformer et FNet qui remplacent l'attention quadratique par des approximations efficaces
- **M√©moire Associative Moderne**: Int√®gre des r√©seaux de Hopfield continus de grande capacit√© pour la m√©morisation associative
- **Routage Adaptatif**: Utilise des techniques de routage dynamique pour activer s√©lectivement diff√©rents "experts" selon le contexte
- **Composants Neurosymboliques (√©tendus)**: Combine traitement neuronal avec des m√©canismes de raisonnement symbolique et probabiliste plus explicites pour am√©liorer les capacit√©s de raisonnement structur√© et la gestion de l'incertitude, tout en maintenant une faible empreinte param√©trique.
- **Apprentissage Continu (Lifelong Learning)**: S'inspire des approches visant √† permettre aux mod√®les d'apprendre s√©quentiellement de nouvelles t√¢ches ou donn√©es sans oublier les pr√©c√©dentes, en utilisant des tampons de rejeu et des m√©canismes d'adaptation.
- **M√©moires Hi√©rarchiques Dynamiques**: Les am√©liorations apport√©es √† la m√©moire s'inspirent des mod√®les cognitifs de la m√©moire humaine, o√π la consolidation et la r√©cup√©ration sont des processus dynamiques et d√©pendants du contexte et de la nouveaut√©.

## üì¶ Structure du Projet

```
neurolite/
‚îú‚îÄ‚îÄ __init__.py        # Point d'entr√©e du package
‚îú‚îÄ‚îÄ config.py          # Configuration des diff√©rentes tailles de mod√®les
‚îú‚îÄ‚îÄ model.py           # Mod√®le principal et variantes sp√©cialis√©es
‚îú‚îÄ‚îÄ projection.py      # Couche de projection d'entr√©e (MinHash+Bloom)
‚îú‚îÄ‚îÄ mixer.py           # Impl√©mentations MLP-Mixer, HyperMixer, FNet
‚îú‚îÄ‚îÄ memory.py          # M√©moire externe diff√©rentiable (base)
‚îú‚îÄ‚îÄ hierarchical_memory.py # M√©moire hi√©rarchique am√©lior√©e
‚îú‚îÄ‚îÄ routing.py         # Routage dynamique et Mixture-of-Experts
‚îú‚îÄ‚îÄ symbolic.py        # Composants de raisonnement symbolique (moteur de r√®gles, couche neurosymbolique, BBN)
‚îî‚îÄ‚îÄ continual.py       # Adaptateur d'apprentissage continu

training/
‚îú‚îÄ‚îÄ data_manager.py    # Gestion des donn√©es d'entra√Ænement et validation
‚îú‚îÄ‚îÄ train_generator.py # Script d'entra√Ænement du mod√®le de g√©n√©ration
‚îî‚îÄ‚îÄ train_classifier.py # Script d'entra√Ænement du classifieur

data/
‚îî‚îÄ‚îÄ wikitext/         # Donn√©es d'entra√Ænement provenant du corpus WikiText
    ‚îú‚îÄ‚îÄ train/        # Donn√©es d'entra√Ænement
    ‚îú‚îÄ‚îÄ val/          # Donn√©es de validation
    ‚îî‚îÄ‚îÄ test/         # Donn√©es de test

examples/
‚îú‚îÄ‚îÄ simple_example.py           # Exemple basique d'utilisation
‚îú‚îÄ‚îÄ classification_example.py   # Classification de texte
‚îú‚îÄ‚îÄ memory_and_routing_example.py # D√©monstration m√©moire et routage
‚îú‚îÄ‚îÄ symbolic_reasoning_example.py # D√©monstration du moteur de r√®gles et couche neurosymbolique
‚îú‚îÄ‚îÄ bayesian_network_example.py   # D√©monstration du r√©seau bay√©sien
‚îú‚îÄ‚îÄ lifelong_learning_demo.py     # D√©monstration de l'apprentissage continu avec l'adaptateur et la m√©moire hi√©rarchique
‚îî‚îÄ‚îÄ benchmark_comparison.py     # Comparaison avec architectures standards

generate_text.py     # Utilitaire de g√©n√©ration de texte avec mod√®le entra√Æn√©
neurolite_demo.py    # Application de d√©monstration interactive
```

## üöÄ Installation

Pour installer les d√©pendances n√©cessaires:

```bash
git clone https://github.com/username/NeuroLite.git
cd NeuroLite
python -m venv .venv
.venv\Scripts\activate  # Sur Windows
source .venv/bin/activate  # Sur Linux/MacOS
pip install -r requirements.txt
```

## üîß Utilisation

### Exemple Simple

```python
from neurolite import NeuroLiteModel, NeuroLiteConfig

# Cr√©er un mod√®le ultra-l√©ger
config = NeuroLiteConfig.tiny()  # ~1-2Mo
model = NeuroLiteModel(config)

# Traiter du texte
texts = ["NeuroLite est une architecture l√©g√®re d'IA."]
outputs = model(input_texts=texts)

# Utiliser les repr√©sentations vectorielles (embeddings)
embedding = outputs.mean(dim=1)
```

### Classification de Texte

```python
from neurolite import NeuroLiteForClassification, NeuroLiteConfig

# Configurer un mod√®le pour classification
config = NeuroLiteConfig.small()
model = NeuroLiteForClassification(config, num_labels=2)

# Inf√©rence
outputs = model(input_texts=["Texte √† classifier"])
prediction = outputs["logits"].argmax(dim=1)
```

### Utilisation de la M√©moire Contextuelle

```python
# Cr√©er un mod√®le avec m√©moire
config = NeuroLiteConfig.small()
config.use_external_memory = True
model = NeuroLiteModel(config)

# Fournir du contexte √† la m√©moire
model(input_texts=["Alice est une ing√©nieure vivant √† Paris."], update_memory=True)

# La requ√™te suivante sera enrichie par le contexte en m√©moire
result = model(input_texts=["O√π habite-t-elle ?"])
```

## üèãÔ∏è Entra√Ænement du Mod√®le

NeuroLite comprend des scripts d'entra√Ænement robustes pour diverses t√¢ches. Pour entra√Æner un mod√®le de g√©n√©ration de texte sur le corpus WikiText :

```bash
python training/train_generator.py --data_dir "data/wikitext" --batch_size 32 --seq_length 512 --vocab_size 32000 --num_epochs 20
```

Options d'entra√Ænement importantes :
- `--batch_size` : Taille des batchs (d√©faut: 32)
- `--seq_length` : Longueur de s√©quence pour l'entra√Ænement (d√©faut: 128)
- `--vocab_size` : Taille du vocabulaire (d√©faut: 10000)
- `--hidden_size` : Dimension des couches cach√©es (d√©faut: 256)
- `--num_layers` : Nombre de couches mixer (d√©faut: 6)
- `--use_memory` : Activer la m√©moire externe (flag)
- `--learning_rate` : Taux d'apprentissage (d√©faut: 5e-5)
- `--max_samples` : Limite le nombre d'√©chantillons (pour tests rapides)

Pour entra√Æner sur un mat√©riel limit√©, utilisez des param√®tres plus l√©gers :

```bash
python training/train_generator.py --data_dir "data/wikitext" --batch_size 8 --seq_length 128 --hidden_size 128 --num_layers 4 --num_epochs 5 --max_samples 1000
```

Une fois entra√Æn√©, g√©n√©rez du texte avec le mod√®le :

```bash
python generate_text.py --model_path "models/generator_ep20.pt" --prompt "NeuroLite est" --max_length 100
```

## üß™ Exemples et D√©monstration

Ex√©cutez la d√©monstration interactive pour explorer les capacit√©s du mod√®le:

```bash
python neurolite_demo.py --size tiny  # Options: tiny, small, base
```

Autres exemples disponibles:
- `examples/simple_example.py` - Utilisation de base
- `examples/classification_example.py` - Classification de sentiment
- `examples/memory_and_routing_example.py` - D√©monstration m√©moire et routage
- `examples/benchmark_comparison.py` - Comparaison avec Transformer

## üìà Performances

Comparaison avec des architectures standards sur un texte de longueur moyenne (256 tokens):

| Architecture | Param√®tres | Temps d'inf√©rence | M√©moire (RAM) | Complexit√© |
|--------------|------------|-------------------|---------------|------------|
| BERT-base    | 110M       | ~45ms             | ~440MB        | O(n¬≤)      |
| DistilBERT   | 66M        | ~25ms             | ~265MB        | O(n¬≤)      |
| NeuroLite-base | 8M       | ~10ms             | ~32MB         | O(n)       |
| NeuroLite-small | 3M      | ~5ms              | ~12MB         | O(n)       |
| NeuroLite-tiny  | 1M      | ~2ms              | ~4MB          | O(n)       |

## üõ†Ô∏è Personnalisation

NeuroLite offre plusieurs configurations pr√©-d√©finies:

```python
# Tr√®s l√©ger (~1-2Mo)
config = NeuroLiteConfig.tiny()

# L√©ger (~5-10Mo)
config = NeuroLiteConfig.small()

# Standard (~20-30Mo)
config = NeuroLiteConfig.base()

# Standard avec modules symboliques et bay√©siens activ√©s par d√©faut
config = NeuroLiteConfig.base_symbolic()
# Cela active `use_symbolic_module=True`, `symbolic_rules_file="rules.json"`,
# `use_bayesian_module=True`, et d√©finit une structure bay√©sienne d'exemple.

# Personnalisation avanc√©e
config = NeuroLiteConfig(
    hidden_size=256,
    num_mixer_layers=6,
    use_external_memory=True,
    use_dynamic_routing=True,
    num_experts=4,
    
    # Activation et configuration du module symbolique
    use_symbolic_module=True,
    symbolic_rules_file="custom_rules.json", # Chemin vers votre fichier de r√®gles
    max_predicate_types=100, # Taille du vocabulaire pour les types de pr√©dicats
    max_entities_in_vocab=500, # Taille du vocabulaire pour les entit√©s symboliques

    # Activation et configuration du r√©seau bay√©sien
    use_bayesian_module=True,
    num_bayesian_variables=15,
    # Exemple: [(parent_idx, child_idx), ...]
    bayesian_network_structure=[(0, 1), (0, 2), (1, 3), (2, 3), (3, 4)], 
    max_parents_bayesian=3, # Utilis√© si bayesian_network_structure n'est pas fourni

    # Activation et configuration de l'apprentissage continu
    use_continual_adapter=True,
    continual_adapter_buffer_size=200, # Taille du tampon de rejeu
    continual_adapter_rate=0.05,       # Taux d'adaptation
    continual_adapter_drift_threshold=0.6, # Seuil de d√©tection de d√©rive

    # Configuration des seuils de nouveaut√© pour HierarchicalMemory
    novelty_threshold_ltm=0.6, # Seuil pour la mise √† jour de la m√©moire √† long terme
    novelty_threshold_pm=0.7   # Seuil pour la mise √† jour de la m√©moire persistante
)
```

## üîÑ Gestion des Donn√©es & Optimisations

NeuroLite comprend des syst√®mes robustes pour la gestion et le traitement des donn√©es :

- **WikiTextDataset** : Chargement efficace et gestion du corpus WikiText
- **Padding intelligent** : Traitement optimal des textes plus courts que la longueur de s√©quence cible
- **Tokenization optimis√©e** : Tokenizer rapide avec vocabulaire ajustable (jusqu'√† 32K tokens)
- **Multiprocessing** : Chargement parall√®le des donn√©es pour acc√©l√©rer l'entra√Ænement
- **Gestion de batch dynamique** : Fonction de collation robuste pour la cr√©ation de batchs homog√®nes
- **Int√©gration PyTorch** : Compatibilit√© compl√®te avec l'√©cosyst√®me PyTorch (DataLoader, etc.)

Chaque composant est con√ßu pour √™tre efficace en m√©moire et en temps de calcul, m√™me sur du mat√©riel limit√©.

## üìö R√©f√©rences

Cette impl√©mentation s'inspire des travaux suivants:
- MLP-Mixer (Tolstikhin et al., 2021)
- pNLP-Mixer (Fusco et al., 2023)
- HyperMixer (Mai et al., 2023)
- FNet (Lee et al., 2022)
- Performer (Choromanski et al., 2020)
- Modern Hopfield Networks (Ramsauer et al., 2020)
- Differentiable Neural Computers (Graves et al., 2016)
- State Space Models (Gu et al., 2023)
- Mamba (Gu et al., 2023)

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de d√©tails.

## ü§ù Contributions

Les contributions sont bienvenues! N'h√©sitez pas √† soumettre des pull requests ou √† ouvrir des issues pour des suggestions d'am√©lioration.

---

<div align="center">
<strong>NeuroLite - Vers une AGI l√©g√®re et efficiente</strong>
</div>
