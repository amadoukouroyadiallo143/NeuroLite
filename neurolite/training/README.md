# ğŸš€ NeuroLite AGI Training v2.0

SystÃ¨me d'entraÃ®nement complet pour NeuroLite AGI avec support modulaire, multi-GPU et mÃ©triques AGI spÃ©cialisÃ©es.

## ğŸƒâ€â™‚ï¸ DÃ©marrage Rapide

### Test Ultra-Rapide (2 minutes)
```bash
# Test configuration (dry run)
python -m neurolite.train_neurolite --dry-run --config tiny

# Test entraÃ®nement (1 batch)
python -m neurolite.train_neurolite --fast-dev-run --config tiny --batch-size 1
```

### EntraÃ®nement Complet Local
```bash
# ModÃ¨le tiny (recommandÃ© pour budget 0â‚¬)
python -m neurolite.train_neurolite \
    --config tiny \
    --epochs 3 \
    --batch-size 2 \
    --dataset-type multimodal \
    --experiment-name "neurolite_tiny_test"
```

### Avec Wandb (optionnel)
```bash
# Avec monitoring Wandb
python -m neurolite.train_neurolite \
    --config tiny \
    --use-wandb \
    --experiment-name "neurolite_wandb_test"
```

## ğŸ“Š Datasets Disponibles

| Dataset | Taille | Description | Utilisation |
|---------|--------|-------------|-------------|
| `multimodal` | 800 Ã©chantillons | Texte + Image + Audio | EntraÃ®nement gÃ©nÃ©ral |
| `text` | 500 sÃ©quences | GÃ©nÃ©ration de texte | Phase gÃ©nÃ©ration |
| `consciousness` | 300 Ã©chantillons | Niveaux de conscience | Module conscience |
| `memory` | 400 Ã©chantillons | Rappel mÃ©moire | Module mÃ©moire |

## ğŸ¯ Phases d'EntraÃ®nement

Le systÃ¨me utilise 5 phases d'entraÃ®nement sÃ©quentielles :

### 1. Foundation (3 Ã©poques)
```bash
# EntraÃ®ne SSM Core + Multimodal Fusion
python -m neurolite.train_neurolite --phase foundation
```

### 2. Cognition (2 Ã©poques) 
```bash
# EntraÃ®ne Cognitive Core
python -m neurolite.train_neurolite --phase cognition
```

### 3. Higher-Order (2 Ã©poques)
```bash
# EntraÃ®ne Consciousness + Memory
python -m neurolite.train_neurolite --phase higher_order
```

### 4. Integration (1 Ã©poque)
```bash
# EntraÃ®ne Reasoning + Interface
python -m neurolite.train_neurolite --phase integration
```

### 5. End-to-End (3 Ã©poques)
```bash
# EntraÃ®nement global de tous les modules
python -m neurolite.train_neurolite --phase end_to_end
```

## âš™ï¸ Configurations

### Tiny (RecommandÃ© - Budget 0â‚¬)
- **ParamÃ¨tres** : ~12M
- **RAM** : 2GB
- **VRAM** : 4GB
- **Temps** : 30min sur CPU, 5min sur GPU

### Development
- **ParamÃ¨tres** : ~25M  
- **RAM** : 4GB
- **VRAM** : 8GB
- **Temps** : 1h sur CPU, 15min sur GPU

## ğŸ”§ Options AvancÃ©es

### Optimisation MÃ©moire
```bash
# Ultra-compact pour machines limitÃ©es
python -m neurolite.train_neurolite \
    --config tiny \
    --batch-size 1 \
    --accumulate-grad-batches 8
```

### GPU Multiple (si disponible)
```bash
# Multi-GPU
python -m neurolite.train_neurolite \
    --gpus 2 \
    --batch-size 4
```

### Debug & Development
```bash
# Profiling activÃ©
python -m neurolite.train_neurolite \
    --config dev \
    --log-every-n-steps 1 \
    --fast-dev-run
```

## ğŸ“ˆ MÃ©triques Suivies

### MÃ©triques GÃ©nÃ©rales
- **Loss** : Perte totale composite
- **Accuracy** : PrÃ©cision approximative 
- **Perplexity** : PerplexitÃ© du modÃ¨le

### MÃ©triques AGI SpÃ©cialisÃ©es
- **Consciousness Coherence** : CohÃ©rence de conscience (0-1)
- **Memory Precision/Recall** : PrÃ©cision mÃ©moire
- **Reasoning Validity** : ValiditÃ© du raisonnement
- **Processing Speed** : Vitesse de traitement

## ğŸ’¾ Checkpoints & Sauvegarde

### Checkpoints Automatiques
- **Meilleur modÃ¨le** : BasÃ© sur validation loss
- **DerniÃ¨re Ã©poque** : Sauvegarde finale
- **Top-3** : 3 meilleurs checkpoints

### Localisation
```
./experiments/results/
â”œâ”€â”€ {experiment_name}/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ best.ckpt
â”‚   â”‚   â”œâ”€â”€ last.ckpt
â”‚   â”‚   â””â”€â”€ epoch_*.ckpt
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ tensorboard/
```

### Chargement Checkpoint
```python
from neurolite.training.trainer import NeuroLiteTrainer

# Charger checkpoint
model = NeuroLiteTrainer.load_from_checkpoint("path/to/checkpoint.ckpt")
```

## ğŸ› Troubleshooting

### Erreur MÃ©moire GPU
```bash
# RÃ©duire batch size
python -m neurolite.train_neurolite --batch-size 1

# Activer CPU offload
python -m neurolite.train_neurolite --config tiny
```

### Slow Training
```bash
# Test avec un seul batch
python -m neurolite.train_neurolite --fast-dev-run

# VÃ©rifier GPU
python -c "import torch; print(torch.cuda.is_available())"
```

### Dataset Error
```bash
# Test dataset seul
python -c "from neurolite.datasets import create_sample_dataloaders; create_sample_dataloaders(batch_size=1)"
```

## ğŸ“Š Monitoring

### TensorBoard (Local)
```bash
# Lancer TensorBoard
tensorboard --logdir ./experiments/results

# Naviguer vers http://localhost:6006
```

### Wandb (Cloud - Optionnel)
```bash
# Setup Wandb
pip install wandb
wandb login

# EntraÃ®nement avec Wandb
python -m neurolite.train_neurolite --use-wandb
```

## ğŸ¯ Cas d'Usage Typiques

### Test Rapide Development
```bash
python -m neurolite.train_neurolite \
    --fast-dev-run \
    --config tiny \
    --batch-size 1
```

### EntraÃ®nement Production Locale
```bash
python -m neurolite.train_neurolite \
    --config tiny \
    --epochs 5 \
    --batch-size 2 \
    --use-wandb \
    --experiment-name "prod_$(date +%Y%m%d)"
```

### EntraÃ®nement Colab
```bash
# Dans Colab
!pip install pytorch-lightning wandb
!python -m neurolite.train_neurolite \
    --config tiny \
    --batch-size 1 \
    --epochs 3 \
    --gpus 1
```

## ğŸ”„ Pipeline Complet

```bash
# 1. Test setup
python -m neurolite.train_neurolite --dry-run

# 2. Test rapide
python -m neurolite.train_neurolite --fast-dev-run

# 3. EntraÃ®nement complet
python -m neurolite.train_neurolite --epochs 5

# 4. Ã‰valuation
python -c "from neurolite.training.metrics import AGIMetrics; print('MÃ©triques calculÃ©es')"
```

## ğŸ“ Logs & Outputs

### Structure des Logs
```
./experiments/
â”œâ”€â”€ results/
â”‚   â””â”€â”€ {experiment}/
â”‚       â”œâ”€â”€ checkpoints/
â”‚       â””â”€â”€ version_*/
â”‚           â””â”€â”€ metrics.csv
â””â”€â”€ logs/
    â””â”€â”€ training.log
```

### MÃ©triques ExportÃ©es
- **CSV** : `metrics.csv` avec toutes les mÃ©triques par step
- **JSON** : RÃ©sumÃ© final de l'expÃ©rience
- **TensorBoard** : Graphiques interactifs

---

## ğŸ‰ Exemple Complet

```bash
# EntraÃ®nement NeuroLite AGI - Configuration optimale budget 0â‚¬
python -m neurolite.train_neurolite \
    --config tiny \
    --dataset-type multimodal \
    --epochs 3 \
    --batch-size 2 \
    --learning-rate 1e-4 \
    --experiment-name "neurolite_budget_zero" \
    --log-every-n-steps 5
```

**Temps estimÃ©** : 15-30 minutes sur CPU, 3-5 minutes sur GPU  
**Ressources** : 2GB RAM, 1GB VRAM  
**RÃ©sultat** : ModÃ¨le AGI fonctionnel de 12M paramÃ¨tres